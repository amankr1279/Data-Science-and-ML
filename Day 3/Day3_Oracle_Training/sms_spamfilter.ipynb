{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca22b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms_spam = pd.read_csv('d:/sms_spam.csv')\n",
    "\n",
    "print(sms_spam.shape)\n",
    "sms_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51b7d08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865985\n",
       "spam    0.134015\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam['type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d72e10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the dataset\n",
    "data_randomized = sms_spam.sample(frac=1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "674c55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate index for split\n",
    "training_test_index = round(len(data_randomized) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "332980a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "training_set = data_randomized[:training_test_index].reset_index(drop=True)\n",
    "test_set = data_randomized[training_test_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6809ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4459, 2)\n",
      "(1115, 2)\n"
     ]
    }
   ],
   "source": [
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b25559b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865216\n",
       "spam    0.134784\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa1139a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.869058\n",
       "spam    0.130942\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c098b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Looks like u wil b getting a headstart im leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>I noe la... U wana pei bf oso rite... K lor, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>2mro i am not coming to gym machan. Goodnight.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text\n",
       "0  ham  Looks like u wil b getting a headstart im leav...\n",
       "1  ham  I noe la... U wana pei bf oso rite... K lor, o...\n",
       "2  ham     2mro i am not coming to gym machan. Goodnight."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before cleaning\n",
    "training_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e70f38c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAKSHMI\\AppData\\Local\\Temp\\ipykernel_20816\\3521266103.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  training_set['text'] = training_set['text'].str.replace(r'\\W', ' ') # Removes punctuation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>looks like u wil b getting a headstart im leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>i noe la    u wana pei bf oso rite    k lor  o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>2mro i am not coming to gym machan  goodnight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text\n",
       "0  ham  looks like u wil b getting a headstart im leav...\n",
       "1  ham  i noe la    u wana pei bf oso rite    k lor  o...\n",
       "2  ham     2mro i am not coming to gym machan  goodnight "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Data clensing\n",
    "training_set['text'] = training_set['text'].str.replace(r'\\W', ' ') # Removes punctuation\n",
    "training_set['text'] = training_set['text'].str.lower()\n",
    "training_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c85920ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncreate the vocabulary\\nWe transform each message in thetext column into a list by splitting the string at the space character â€” we're using the Series.str.split() method.\\nWe initiate an empty list named vocabulary.\\nWe iterate over the transformed text column.\\nUsing a nested loop, we iterate over each message in the text column and append each string (word) to the vocabulary list.\\nWe transform the vocabulary list into a set using the set() function. This will remove the duplicates from the vocabulary list.\\nWe transform the vocabulary set back into a list using the list() function.\\n\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "create the vocabulary\n",
    "We transform each message in thetext column into a list by splitting the string at the space character â€” we're using the Series.str.split() method.\n",
    "We initiate an empty list named vocabulary.\n",
    "We iterate over the transformed text column.\n",
    "Using a nested loop, we iterate over each message in the text column and append each string (word) to the vocabulary list.\n",
    "We transform the vocabulary list into a set using the set() function. This will remove the duplicates from the vocabulary list.\n",
    "We transform the vocabulary set back into a list using the list() function.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13dda267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_set['text'] = training_set['text'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for text in training_set['text']:\n",
    "   for word in text:\n",
    "      vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e09a74f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7802"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d959e510",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gower',\n",
       " 'eldest',\n",
       " 'luck',\n",
       " 'watching',\n",
       " '083',\n",
       " 'meetins',\n",
       " 'took',\n",
       " 'football',\n",
       " 'iraq',\n",
       " 'diesel',\n",
       " 'peaceful',\n",
       " 'frontierville',\n",
       " 'gym',\n",
       " 'contract',\n",
       " 'nok',\n",
       " 'b4utele',\n",
       " 'swatch',\n",
       " 'long',\n",
       " '47',\n",
       " 'receipts',\n",
       " 'sky',\n",
       " 'more',\n",
       " 'tears',\n",
       " 'txting',\n",
       " 'infections',\n",
       " 'fold',\n",
       " 'waqt',\n",
       " 'gamb',\n",
       " 'temp',\n",
       " '89555',\n",
       " 'srt',\n",
       " 'crab',\n",
       " 'gentle',\n",
       " 'shitin',\n",
       " 'theacusations',\n",
       " 'l8rs',\n",
       " 'smiley',\n",
       " 'chief',\n",
       " 'keyword',\n",
       " 'tones',\n",
       " 'doors',\n",
       " 'predictive',\n",
       " 'press',\n",
       " 'burger',\n",
       " '12',\n",
       " 'backdoor',\n",
       " 'wonder',\n",
       " 'funny',\n",
       " 'id',\n",
       " '523',\n",
       " 'arrival',\n",
       " 'nat',\n",
       " 'tootsie',\n",
       " 'banter',\n",
       " 'cum',\n",
       " 'boo',\n",
       " 'partner',\n",
       " 'base',\n",
       " 'nr31',\n",
       " 'april',\n",
       " 'uawake',\n",
       " 'map',\n",
       " 'billed',\n",
       " 'maturity',\n",
       " 'difference',\n",
       " 'advice',\n",
       " 'comedy',\n",
       " 'wins',\n",
       " 'hard',\n",
       " 'haiz',\n",
       " 'swt',\n",
       " 'theirs',\n",
       " 'tenants',\n",
       " 'matter',\n",
       " 'darlings',\n",
       " 'application',\n",
       " 'caller',\n",
       " 'oz',\n",
       " 'muah',\n",
       " 'shola',\n",
       " 'big',\n",
       " '08706091795',\n",
       " 'cashto',\n",
       " 'timi',\n",
       " 'delhi',\n",
       " 'chez',\n",
       " 'answered',\n",
       " 'leona',\n",
       " 'roads',\n",
       " 'needle',\n",
       " 'wrc',\n",
       " 'entry',\n",
       " 'child',\n",
       " '62468',\n",
       " 'business',\n",
       " 'dict',\n",
       " 'enemy',\n",
       " 'les',\n",
       " 'lay',\n",
       " 'jersey',\n",
       " 'fro',\n",
       " 'from',\n",
       " 'overa',\n",
       " 'superb',\n",
       " 'phd',\n",
       " 'mystery',\n",
       " '087187272008',\n",
       " 'tog',\n",
       " 'pandy',\n",
       " '88600',\n",
       " '1pm',\n",
       " 'olympics',\n",
       " 'wonders',\n",
       " 'steering',\n",
       " 'flung',\n",
       " 'kfc',\n",
       " 'entire',\n",
       " 'meg',\n",
       " 'influx',\n",
       " 'anniversary',\n",
       " 'didnt',\n",
       " 'a30',\n",
       " 'darling',\n",
       " 'blame',\n",
       " 'okors',\n",
       " '4few',\n",
       " 'ringing',\n",
       " 'always',\n",
       " 'operate',\n",
       " 'swimming',\n",
       " 'sponsors',\n",
       " 'dancing',\n",
       " 'basket',\n",
       " 'prince',\n",
       " 'leg',\n",
       " 'cantdo',\n",
       " 'doublemins',\n",
       " 'toothpaste',\n",
       " 'hopeing',\n",
       " 'watts',\n",
       " '69866',\n",
       " 'body',\n",
       " 'matric',\n",
       " 'blank',\n",
       " 'irulinae',\n",
       " 'mis',\n",
       " 'killing',\n",
       " 'props',\n",
       " 'chosen',\n",
       " 'heehee',\n",
       " 'spelling',\n",
       " 'turned',\n",
       " 'txtin',\n",
       " 'mens',\n",
       " 'doubletxt',\n",
       " 'thankyou',\n",
       " 'seing',\n",
       " 'golddigger',\n",
       " 'vip',\n",
       " 'no1',\n",
       " 'mood',\n",
       " 'nokia6600',\n",
       " '400thousad',\n",
       " 'fletcher',\n",
       " 'taking',\n",
       " 'laundry',\n",
       " '50p',\n",
       " 'suckers',\n",
       " 'dangerous',\n",
       " 'appreciate',\n",
       " 'yalrigu',\n",
       " 'repent',\n",
       " 'card',\n",
       " 'ext',\n",
       " 'cha',\n",
       " 'gurl',\n",
       " 'sorrow',\n",
       " 'celebration',\n",
       " 'balls',\n",
       " 'silence',\n",
       " 'resume',\n",
       " 'hlday',\n",
       " 'wicket',\n",
       " 'foregate',\n",
       " 'natural',\n",
       " 'panasonic',\n",
       " 'argh',\n",
       " 'tmw',\n",
       " '07946746291',\n",
       " 'join',\n",
       " 'showered',\n",
       " '0quit',\n",
       " 'bac',\n",
       " 'dined',\n",
       " '1146',\n",
       " 'tirunelvali',\n",
       " 'august',\n",
       " 'pg',\n",
       " 'path',\n",
       " 'phone750',\n",
       " 'project',\n",
       " 'thew',\n",
       " '31p',\n",
       " '80',\n",
       " 'ltdhelpdesk',\n",
       " 'buying',\n",
       " 'sol',\n",
       " 'bedbut',\n",
       " 'kindly',\n",
       " 'twilight',\n",
       " 'stated',\n",
       " 'sporadically',\n",
       " 'aha',\n",
       " 'online',\n",
       " 'wahleykkum',\n",
       " 'professors',\n",
       " 'uncle',\n",
       " 'age16',\n",
       " 'treasure',\n",
       " 'bani',\n",
       " 'across',\n",
       " 'rushing',\n",
       " 'tasts',\n",
       " 'swap',\n",
       " 'ill',\n",
       " 'ericsson',\n",
       " 'hide',\n",
       " 'sisters',\n",
       " 'house',\n",
       " '3aj',\n",
       " 'fluids',\n",
       " 'useful',\n",
       " 'minor',\n",
       " 'vijaykanth',\n",
       " 'mi',\n",
       " 'replied',\n",
       " 'aunts',\n",
       " 'dysentry',\n",
       " 'havbeen',\n",
       " 'tomeandsaid',\n",
       " 'sudn',\n",
       " 'lonely',\n",
       " 'flying',\n",
       " 'method',\n",
       " 'heading',\n",
       " 'cherthala',\n",
       " 'north',\n",
       " 'lt',\n",
       " 'appeal',\n",
       " '37819',\n",
       " 'apo',\n",
       " 'selfish',\n",
       " 'alternative',\n",
       " 'uh',\n",
       " 'month',\n",
       " 'musical',\n",
       " 'regular',\n",
       " 'nitros',\n",
       " 'honestly',\n",
       " 'unsecured',\n",
       " 'dnt',\n",
       " 'cuddling',\n",
       " 'desperate',\n",
       " 'yorge',\n",
       " 'aldrine',\n",
       " 'ideas',\n",
       " 'weirdest',\n",
       " 'cook',\n",
       " 'gymnastics',\n",
       " 'ccna',\n",
       " '69888',\n",
       " 'rimac',\n",
       " 'slap',\n",
       " 'bucks',\n",
       " 'inch',\n",
       " 'uv',\n",
       " 'kl341',\n",
       " 'onum',\n",
       " 'mnth',\n",
       " 'leading',\n",
       " 'lotto',\n",
       " 'hour',\n",
       " 'answerin',\n",
       " 'prsn',\n",
       " 'dark',\n",
       " 'felt',\n",
       " 'goverment',\n",
       " 'fights',\n",
       " 'commercial',\n",
       " 'limping',\n",
       " '6times',\n",
       " 'psp',\n",
       " '130',\n",
       " 'bears',\n",
       " 'murdered',\n",
       " 'tough',\n",
       " '08081263000',\n",
       " '50gbp',\n",
       " 'taxless',\n",
       " 'login',\n",
       " 'feb',\n",
       " 'maybe',\n",
       " 'imp',\n",
       " 'gas',\n",
       " 'swtheart',\n",
       " '447797706009',\n",
       " 'stones',\n",
       " 'getsleep',\n",
       " 'battle',\n",
       " 'iq',\n",
       " '09058091854',\n",
       " 'wa',\n",
       " '09066361921',\n",
       " '18p',\n",
       " '08717111821',\n",
       " 'emergency',\n",
       " 'lux',\n",
       " 'mp3',\n",
       " 'pillows',\n",
       " 'appear',\n",
       " 'reserved',\n",
       " 'nino',\n",
       " 'keeps',\n",
       " 'bajarangabali',\n",
       " 'shouted',\n",
       " 'nelson',\n",
       " 'refunded',\n",
       " 'fatty',\n",
       " 'citylink',\n",
       " 'itz',\n",
       " 'pract',\n",
       " 'm263uz',\n",
       " 'licks',\n",
       " 'buttons',\n",
       " 'log',\n",
       " 'series',\n",
       " 'called',\n",
       " 'sundayish',\n",
       " 'sh',\n",
       " 'hurricanes',\n",
       " 'urgnt',\n",
       " '449050000301',\n",
       " 'breathe1',\n",
       " 'earn',\n",
       " '82277',\n",
       " 'receivea',\n",
       " 'fall',\n",
       " 'suddenly',\n",
       " 'hills',\n",
       " 'getzed',\n",
       " 'stool',\n",
       " 'balance',\n",
       " 'living',\n",
       " 'fine',\n",
       " '1st',\n",
       " 'terrorist',\n",
       " 'potter',\n",
       " 'elaborating',\n",
       " 'imat',\n",
       " 'dudes',\n",
       " 'compulsory',\n",
       " 'pounds',\n",
       " 'professional',\n",
       " 'samus',\n",
       " 'anyway',\n",
       " '08712402779',\n",
       " 'cthen',\n",
       " 'wit',\n",
       " 'brandy',\n",
       " 'helpline',\n",
       " 'sexual',\n",
       " 'console',\n",
       " 'active',\n",
       " 'newsletter',\n",
       " 'tmorrow',\n",
       " 'holla',\n",
       " 'sips',\n",
       " 'english',\n",
       " 'bakra',\n",
       " 'goggles',\n",
       " 'acknowledgement',\n",
       " 'soil',\n",
       " 'eveb',\n",
       " 'photoshop',\n",
       " 'info',\n",
       " 'dave',\n",
       " 'jack',\n",
       " 'zeros',\n",
       " 'playin',\n",
       " 'bank',\n",
       " 'pop',\n",
       " 'die',\n",
       " 'sipix',\n",
       " 'dogging',\n",
       " 'Ã¼',\n",
       " 'admin',\n",
       " 'kind',\n",
       " 'irritated',\n",
       " 'erotic',\n",
       " 'scary',\n",
       " 'still',\n",
       " 'aunt',\n",
       " 'tncs',\n",
       " '1680',\n",
       " 'bec',\n",
       " 'senor',\n",
       " 'tihs',\n",
       " 'winds',\n",
       " 'link',\n",
       " 'lover',\n",
       " 'stupid',\n",
       " 'knock',\n",
       " 'babygoodbye',\n",
       " 'pale',\n",
       " 'kip',\n",
       " 'powerful',\n",
       " 'perhaps',\n",
       " 'caroline',\n",
       " 'slowing',\n",
       " 'honest',\n",
       " 'king',\n",
       " 'religiously',\n",
       " 'messages',\n",
       " 'gone',\n",
       " 'blind',\n",
       " 'prakesh',\n",
       " 'painful',\n",
       " 'ph',\n",
       " 'books',\n",
       " 'hitter',\n",
       " 'apnt',\n",
       " 'callcost150ppmmobilesvary',\n",
       " 'maximum',\n",
       " 'yahoo',\n",
       " 'disconnect',\n",
       " 'ip',\n",
       " 'onlyfound',\n",
       " 'cnupdates',\n",
       " 'depressed',\n",
       " 'oic',\n",
       " 'lrg',\n",
       " 'assessment',\n",
       " 'bites',\n",
       " 'admit',\n",
       " 'sabarish',\n",
       " 'hmm',\n",
       " '80182',\n",
       " 'vco',\n",
       " 'psxtra',\n",
       " 'algarve',\n",
       " 'lololo',\n",
       " 'types',\n",
       " 'reunion',\n",
       " 'rebooting',\n",
       " 'paris',\n",
       " 'colin',\n",
       " 'stylish',\n",
       " 'bc',\n",
       " 'during',\n",
       " 'ppl',\n",
       " 'maps',\n",
       " '09064017305',\n",
       " 'bakrid',\n",
       " 'havebeen',\n",
       " '85069',\n",
       " 'acc',\n",
       " '50ea',\n",
       " 'disclose',\n",
       " 'motive',\n",
       " 'mufti',\n",
       " 'c',\n",
       " 'ethnicity',\n",
       " 'decking',\n",
       " 'bluetooth',\n",
       " 'me',\n",
       " '6days',\n",
       " 'vodafone',\n",
       " '08712101358',\n",
       " 'mt',\n",
       " 'tata',\n",
       " 'double',\n",
       " 'shall',\n",
       " 'taxi',\n",
       " 'gotten',\n",
       " '1030',\n",
       " 'frnt',\n",
       " 'blonde',\n",
       " 'gon',\n",
       " 'transfred',\n",
       " 'aint',\n",
       " 'dialling',\n",
       " 'paths',\n",
       " 'administrator',\n",
       " 'honey',\n",
       " 'luvs',\n",
       " 'quizclub',\n",
       " 'er',\n",
       " 'respect',\n",
       " 'onwards',\n",
       " 'lk',\n",
       " 'mmmm',\n",
       " 'spun',\n",
       " 'sway',\n",
       " 'jos',\n",
       " 'lovin',\n",
       " '008704050406',\n",
       " 'firefox',\n",
       " '1500',\n",
       " 'make',\n",
       " 'preferably',\n",
       " 'souveniers',\n",
       " 'dehydration',\n",
       " 'movie',\n",
       " 'wining',\n",
       " '08712317606',\n",
       " 'carolina',\n",
       " 'shld',\n",
       " 'increase',\n",
       " 'effects',\n",
       " 'girls',\n",
       " '2stoptxt',\n",
       " 'foley',\n",
       " 'song',\n",
       " 'oveable',\n",
       " 'electricity',\n",
       " '83383',\n",
       " 'aom',\n",
       " 'musicnews',\n",
       " '30pm',\n",
       " 'serious',\n",
       " 'ofstuff',\n",
       " 'lord',\n",
       " 'q',\n",
       " 'horny',\n",
       " 'max10mins',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'bullshit',\n",
       " 'fantastic',\n",
       " 'soundtrack',\n",
       " 'flurries',\n",
       " '88066',\n",
       " 'amt',\n",
       " 'app',\n",
       " 'pierre',\n",
       " 'amrca',\n",
       " '09099726429',\n",
       " 'flowers',\n",
       " 'concerned',\n",
       " '87239',\n",
       " 'silly',\n",
       " 'wanting',\n",
       " 'huge',\n",
       " 'ppm150',\n",
       " 'csbcm4235wc1n3xx',\n",
       " 'man',\n",
       " 'ignore',\n",
       " 'armand',\n",
       " 'awkward',\n",
       " '08707509020',\n",
       " 'blocked',\n",
       " '08718726270',\n",
       " 'restock',\n",
       " 'goodfriend',\n",
       " 'wipe',\n",
       " 'twittering',\n",
       " 'nigro',\n",
       " 'othrs',\n",
       " 'previous',\n",
       " 'management',\n",
       " 'hmmm',\n",
       " 'unsubscribe',\n",
       " 'finished',\n",
       " 'witout',\n",
       " 'crore',\n",
       " 'excited',\n",
       " 'neva',\n",
       " 'jeans',\n",
       " 'annie',\n",
       " 'ugh',\n",
       " 'booty',\n",
       " 'ridden',\n",
       " 'shagged',\n",
       " 'text',\n",
       " 'kept',\n",
       " 'goigng',\n",
       " 'chennai',\n",
       " 'safe',\n",
       " 'stress',\n",
       " '8',\n",
       " '3000',\n",
       " 'hot',\n",
       " 'evil',\n",
       " '09058094455',\n",
       " 'toking',\n",
       " 'cu',\n",
       " 'handing',\n",
       " 'petexxx',\n",
       " 'liquor',\n",
       " 'listen',\n",
       " 'todays',\n",
       " 'cer',\n",
       " 'ipads',\n",
       " 'otbox',\n",
       " 'happened',\n",
       " 'shorter',\n",
       " 'skype',\n",
       " 'perf',\n",
       " 'voda',\n",
       " 'gaytextbuddy',\n",
       " 'ghodbandar',\n",
       " 'rhode',\n",
       " 'sankranti',\n",
       " '7250',\n",
       " 'nit',\n",
       " 'lil',\n",
       " 'chatting',\n",
       " 'ericson',\n",
       " 'friendship',\n",
       " '2optout',\n",
       " 'quiteamuzing',\n",
       " 'asking',\n",
       " 'thought',\n",
       " 'respectful',\n",
       " 'close',\n",
       " 'offc',\n",
       " '88039',\n",
       " 'provided',\n",
       " 'interfued',\n",
       " 'stopbcm',\n",
       " 'sozi',\n",
       " 'wewa',\n",
       " 'hv',\n",
       " 'anyhow',\n",
       " 'availa',\n",
       " 'write',\n",
       " 'dieting',\n",
       " 'penny',\n",
       " 'clothes',\n",
       " 'converter',\n",
       " 'gumby',\n",
       " 'statement',\n",
       " '2nights',\n",
       " 'magical',\n",
       " 'freephone',\n",
       " 'finishd',\n",
       " 'whats',\n",
       " 'fifa',\n",
       " 'dearer',\n",
       " 'downloads',\n",
       " 'speed',\n",
       " 'bell',\n",
       " 'job',\n",
       " 'str8',\n",
       " 'kb',\n",
       " 'goodnite',\n",
       " 'hack',\n",
       " 'frauds',\n",
       " 'somewhere',\n",
       " 'b',\n",
       " 'rply',\n",
       " 'penis',\n",
       " 'digital',\n",
       " 'laden',\n",
       " 'director',\n",
       " 'charles',\n",
       " 'issues',\n",
       " 'affection',\n",
       " '4msgs',\n",
       " 'anythingtomorrow',\n",
       " 'republic',\n",
       " '08714342399',\n",
       " 'loving',\n",
       " 'okie',\n",
       " '09066660100',\n",
       " 'bcm1896wc1n3xx',\n",
       " 'mila',\n",
       " 'fm',\n",
       " 'theory',\n",
       " 'coulda',\n",
       " 'apples',\n",
       " 'toplay',\n",
       " 'squishy',\n",
       " '_',\n",
       " 'romcapspam',\n",
       " 'camp',\n",
       " 'shb',\n",
       " 'dozens',\n",
       " 'girlfrnd',\n",
       " 'cultures',\n",
       " 'woah',\n",
       " '04',\n",
       " 'clubmoby',\n",
       " 'mmmmmm',\n",
       " 'player',\n",
       " 'banks',\n",
       " 'bottom',\n",
       " 'kills',\n",
       " 'odi',\n",
       " 'xx',\n",
       " 'fps',\n",
       " 'system',\n",
       " 'amanda',\n",
       " 'dsn',\n",
       " 'like',\n",
       " 'oops',\n",
       " 'based',\n",
       " '8pm',\n",
       " 'stuff',\n",
       " 'strain',\n",
       " 'bcaz',\n",
       " '7732584351',\n",
       " '151',\n",
       " 'driver',\n",
       " 'ned',\n",
       " 'asa',\n",
       " 'snickering',\n",
       " 'wknd',\n",
       " 'vague',\n",
       " 'usc',\n",
       " 'technical',\n",
       " 'gr8prizes',\n",
       " 'hunks',\n",
       " 'winner',\n",
       " 'revision',\n",
       " 'soiree',\n",
       " 'web2mobile',\n",
       " 'where',\n",
       " 'pussy',\n",
       " '8800',\n",
       " '25',\n",
       " 'bray',\n",
       " 'mfl',\n",
       " 'directly',\n",
       " 'goin2bed',\n",
       " 'salt',\n",
       " 'lotr',\n",
       " 'persevered',\n",
       " 'noi',\n",
       " 'cross',\n",
       " 'gives',\n",
       " '1013',\n",
       " 'limiting',\n",
       " 'whole',\n",
       " 'possibility',\n",
       " 'randomlly',\n",
       " 'lovingly',\n",
       " 'tooth',\n",
       " '36504',\n",
       " 'wesley',\n",
       " 'throws',\n",
       " 'selling',\n",
       " 'brown',\n",
       " 'sef',\n",
       " 'jewelry',\n",
       " 'driving',\n",
       " 'logon',\n",
       " 'birth',\n",
       " 'september',\n",
       " 'adding',\n",
       " 'gari',\n",
       " 'seeing',\n",
       " 'cnl',\n",
       " 'key',\n",
       " 'ese',\n",
       " 'visitor',\n",
       " 'leafcutter',\n",
       " 'daily',\n",
       " '80488',\n",
       " 'clas',\n",
       " 'helps',\n",
       " 'mobno',\n",
       " 'xxxxxxxx',\n",
       " 'glasgow',\n",
       " 'webadres',\n",
       " 'ducking',\n",
       " 'jurong',\n",
       " 'r',\n",
       " 'shakara',\n",
       " 'hear',\n",
       " '09050001295',\n",
       " 'dogg',\n",
       " 'mid',\n",
       " 'didntgive',\n",
       " '4years',\n",
       " 'le',\n",
       " 'shitload',\n",
       " '89080',\n",
       " 'nitw',\n",
       " 'carry',\n",
       " 'lei',\n",
       " '69988',\n",
       " 'cosign',\n",
       " 'planning',\n",
       " '10ppm',\n",
       " 'unconsciously',\n",
       " 'sariyag',\n",
       " 'samantha',\n",
       " 'night',\n",
       " 'bari',\n",
       " 'bruv',\n",
       " 'tc',\n",
       " 'boundaries',\n",
       " 'wild',\n",
       " 'hopeso',\n",
       " 'lubly',\n",
       " 'constantly',\n",
       " 'customer',\n",
       " 'predicting',\n",
       " 'locations',\n",
       " 'someonone',\n",
       " 'cl',\n",
       " '86688',\n",
       " 'nobbing',\n",
       " 'exactly',\n",
       " 'review',\n",
       " 'sambar',\n",
       " 'dodda',\n",
       " 'slice',\n",
       " '2004',\n",
       " 'asian',\n",
       " 'ettans',\n",
       " 'module',\n",
       " 'traveling',\n",
       " 'amplikater',\n",
       " '75',\n",
       " 'thank',\n",
       " 'katexxx',\n",
       " 'pleased',\n",
       " 'anyplaces',\n",
       " '6pm',\n",
       " 'stone',\n",
       " 'unbelievable',\n",
       " 'tiny',\n",
       " 'wishing',\n",
       " 'huh',\n",
       " 'enough',\n",
       " 'named',\n",
       " 'dane',\n",
       " 'remb',\n",
       " 'edge',\n",
       " 'nattil',\n",
       " 'mums',\n",
       " '69855',\n",
       " 'ovarian',\n",
       " 'national',\n",
       " 'jap',\n",
       " 'hcl',\n",
       " 'plz',\n",
       " 'rt',\n",
       " 'mittelschmertz',\n",
       " 'medicine',\n",
       " 'iam',\n",
       " 'fuckinnice',\n",
       " 'box',\n",
       " 'needs',\n",
       " 'smokes',\n",
       " '23f',\n",
       " 'soup',\n",
       " 'kick',\n",
       " 'gosh',\n",
       " 'rounds',\n",
       " 'wondarfull',\n",
       " 'psychiatrist',\n",
       " 'reference',\n",
       " '09050001808',\n",
       " 'yunny',\n",
       " 'lap',\n",
       " 'housework',\n",
       " 'mel',\n",
       " 'coincidence',\n",
       " 'hunting',\n",
       " 'eerie',\n",
       " 'select',\n",
       " 'w1j6hl',\n",
       " 'winning',\n",
       " 'readers',\n",
       " 'feel',\n",
       " 'oble',\n",
       " 'derp',\n",
       " 'mquiz',\n",
       " 'japanese',\n",
       " 'avatar',\n",
       " '10p',\n",
       " 'coffee',\n",
       " 'parkin',\n",
       " 'cdgt',\n",
       " 'merry',\n",
       " 'sory',\n",
       " 'pizza',\n",
       " 'weak',\n",
       " 'reach',\n",
       " 'limit',\n",
       " 'understood',\n",
       " 'ttyl',\n",
       " 'fne',\n",
       " 'spell',\n",
       " 'tonsolitusaswell',\n",
       " 'orh',\n",
       " 'small',\n",
       " '08719181513',\n",
       " 'treacle',\n",
       " 'failure',\n",
       " '255',\n",
       " 'go2',\n",
       " 'textand',\n",
       " 'hella',\n",
       " 'shite',\n",
       " 'threw',\n",
       " '6089',\n",
       " 'access',\n",
       " 'poking',\n",
       " 'pobox114',\n",
       " 'bong',\n",
       " 'pin',\n",
       " 'while',\n",
       " 'demand',\n",
       " 'aiyo',\n",
       " 'summer',\n",
       " '08712402050',\n",
       " 'ones',\n",
       " '08712400602450p',\n",
       " '69696',\n",
       " 'w1jhl',\n",
       " 'opted',\n",
       " '09061701461',\n",
       " 'drugdealer',\n",
       " 'executive',\n",
       " 'zoe',\n",
       " 'stuffs',\n",
       " 'abdomen',\n",
       " 'passed',\n",
       " 'unspoken',\n",
       " 'start',\n",
       " 'stomach',\n",
       " 'cheap',\n",
       " 'cumin',\n",
       " 'shortcode',\n",
       " 'pix',\n",
       " 'that2worzels',\n",
       " 'ger',\n",
       " 'dint',\n",
       " 'naked',\n",
       " 'sathya',\n",
       " 'tiger',\n",
       " 'wrongly',\n",
       " 'msn',\n",
       " 'lived',\n",
       " 'sea',\n",
       " 'fundamentals',\n",
       " 'quoting',\n",
       " 'ogunrinde',\n",
       " 'buz',\n",
       " 'tonght',\n",
       " 'bcums',\n",
       " 'staying',\n",
       " 'videosound',\n",
       " 'saturday',\n",
       " 'stock',\n",
       " 'catches',\n",
       " 'fox',\n",
       " 'lik',\n",
       " 'maangalyam',\n",
       " 'evening',\n",
       " 'dorothy',\n",
       " 'joined',\n",
       " 'cramps',\n",
       " 'urawinner',\n",
       " 'confidence',\n",
       " 'parked',\n",
       " 'anand',\n",
       " 'thk',\n",
       " 'lick',\n",
       " 'monthlysubscription',\n",
       " 'events',\n",
       " 'weakness',\n",
       " 'presents',\n",
       " 'exeter',\n",
       " 'remains',\n",
       " 'knew',\n",
       " 'wt',\n",
       " 'gastroenteritis',\n",
       " 'married',\n",
       " 'truck',\n",
       " '08715203652',\n",
       " 'invitation',\n",
       " 'zindgi',\n",
       " 'prepayment',\n",
       " 'comfey',\n",
       " 'mins',\n",
       " 'tree',\n",
       " 'luvd',\n",
       " 'career',\n",
       " '09066368753',\n",
       " 'days',\n",
       " 'frmcloud',\n",
       " 'y',\n",
       " 'loss',\n",
       " 'welp',\n",
       " 'previews',\n",
       " ...]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b5d62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_text = {'secret': [2,1,1],\n",
    "                       'prize': [2,0,1],\n",
    "                       'claim': [1,0,1],\n",
    "                       'now': [1,0,1],\n",
    "                       'coming': [0,1,0],\n",
    "                       'to': [0,1,0],\n",
    "                       'my': [0,1,0],\n",
    "                       'party': [0,1,0],\n",
    "                       'winner': [0,0,1]\n",
    "                      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cb21707c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'secret': [2, 1, 1],\n",
       " 'prize': [2, 0, 1],\n",
       " 'claim': [1, 0, 1],\n",
       " 'now': [1, 0, 1],\n",
       " 'coming': [0, 1, 0],\n",
       " 'to': [0, 1, 0],\n",
       " 'my': [0, 1, 0],\n",
       " 'party': [0, 1, 0],\n",
       " 'winner': [0, 0, 1]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "694802fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>secret</th>\n",
       "      <th>prize</th>\n",
       "      <th>claim</th>\n",
       "      <th>now</th>\n",
       "      <th>coming</th>\n",
       "      <th>to</th>\n",
       "      <th>my</th>\n",
       "      <th>party</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   secret  prize  claim  now  coming  to  my  party  winner\n",
       "0       2      2      1    1       0   0   0      0       0\n",
       "1       1      0      0    0       1   1   1      1       0\n",
       "2       1      1      1    1       0   0   0      0       1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_text)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d76a2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo create the dictionary we need for our training set\\n\\nWe start by initializing a dictionary named word_counts_per_text, where each key is a\\nunique word (a string) from the vocabulary, and each value is a list of the length of the training set, \\nwhere each element in that list is a 0.\\nThe code [0] * 5 outputs [0, 0, 0, 0, 0]. So the code [0] * len(training_set['text']) outputs a list of the \\nlength of training_set['text'].\\nWe loop over training_set['text'] using the enumerate() function to get both the index and the text message (index and text).\\nUsing a nested loop, we loop over text (where text is a list of strings, where each string represents a word in a message).\\nWe increment word_counts_per_text[word][index] by 1.\\n\\nword_counts_per_text = {unique_word: [0] * len(training_set['text']) for unique_word in vocabulary}\\n\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To create the dictionary we need for our training set\n",
    "\n",
    "We start by initializing a dictionary named word_counts_per_text, where each key is a\n",
    "unique word (a string) from the vocabulary, and each value is a list of the length of the training set, \n",
    "where each element in that list is a 0.\n",
    "The code [0] * 5 outputs [0, 0, 0, 0, 0]. So the code [0] * len(training_set['text']) outputs a list of the \n",
    "length of training_set['text'].\n",
    "We loop over training_set['text'] using the enumerate() function to get both the index and the text message (index and text).\n",
    "Using a nested loop, we loop over text (where text is a list of strings, where each string represents a word in a message).\n",
    "We increment word_counts_per_text[word][index] by 1.\n",
    "\n",
    "word_counts_per_text = {unique_word: [0] * len(training_set['text']) for unique_word in vocabulary}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "489e3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training_set['text']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_set['text']):\n",
    "   for word in sms:\n",
    "      word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e7e6850",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gower</th>\n",
       "      <th>eldest</th>\n",
       "      <th>luck</th>\n",
       "      <th>watching</th>\n",
       "      <th>083</th>\n",
       "      <th>meetins</th>\n",
       "      <th>took</th>\n",
       "      <th>football</th>\n",
       "      <th>iraq</th>\n",
       "      <th>diesel</th>\n",
       "      <th>...</th>\n",
       "      <th>cann</th>\n",
       "      <th>lotsof</th>\n",
       "      <th>creep</th>\n",
       "      <th>devouring</th>\n",
       "      <th>verify</th>\n",
       "      <th>others</th>\n",
       "      <th>facebook</th>\n",
       "      <th>workage</th>\n",
       "      <th>pool</th>\n",
       "      <th>unjalur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gower  eldest  luck  watching  083  meetins  took  football  iraq  diesel  \\\n",
       "0      0       0     0         0    0        0     0         0     0       0   \n",
       "1      0       0     0         0    0        0     0         0     0       0   \n",
       "2      0       0     0         0    0        0     0         0     0       0   \n",
       "3      0       0     0         0    0        0     0         0     0       0   \n",
       "4      0       0     0         0    0        0     0         0     0       0   \n",
       "\n",
       "   ...  cann  lotsof  creep  devouring  verify  others  facebook  workage  \\\n",
       "0  ...     0       0      0          0       0       0         0        0   \n",
       "1  ...     0       0      0          0       0       0         0        0   \n",
       "2  ...     0       0      0          0       0       0         0        0   \n",
       "3  ...     0       0      0          0       0       0         0        0   \n",
       "4  ...     0       0      0          0       0       0         0        0   \n",
       "\n",
       "   pool  unjalur  \n",
       "0     0        0  \n",
       "1     0        0  \n",
       "2     0        0  \n",
       "3     0        0  \n",
       "4     0        0  \n",
       "\n",
       "[5 rows x 7802 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "58c93a0d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>gower</th>\n",
       "      <th>eldest</th>\n",
       "      <th>luck</th>\n",
       "      <th>watching</th>\n",
       "      <th>083</th>\n",
       "      <th>meetins</th>\n",
       "      <th>took</th>\n",
       "      <th>football</th>\n",
       "      <th>...</th>\n",
       "      <th>cann</th>\n",
       "      <th>lotsof</th>\n",
       "      <th>creep</th>\n",
       "      <th>devouring</th>\n",
       "      <th>verify</th>\n",
       "      <th>others</th>\n",
       "      <th>facebook</th>\n",
       "      <th>workage</th>\n",
       "      <th>pool</th>\n",
       "      <th>unjalur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[looks, like, u, wil, b, getting, a, headstart...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, noe, la, u, wana, pei, bf, oso, rite, k, l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[2mro, i, am, not, coming, to, gym, machan, go...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>[todays, vodafone, numbers, ending, with, 4882...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[hi, hope, ur, day, good, back, from, walk, ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7804 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text  gower  eldest  \\\n",
       "0   ham  [looks, like, u, wil, b, getting, a, headstart...      0       0   \n",
       "1   ham  [i, noe, la, u, wana, pei, bf, oso, rite, k, l...      0       0   \n",
       "2   ham  [2mro, i, am, not, coming, to, gym, machan, go...      0       0   \n",
       "3  spam  [todays, vodafone, numbers, ending, with, 4882...      0       0   \n",
       "4   ham  [hi, hope, ur, day, good, back, from, walk, ta...      0       0   \n",
       "\n",
       "   luck  watching  083  meetins  took  football  ...  cann  lotsof  creep  \\\n",
       "0     0         0    0        0     0         0  ...     0       0      0   \n",
       "1     0         0    0        0     0         0  ...     0       0      0   \n",
       "2     0         0    0        0     0         0  ...     0       0      0   \n",
       "3     0         0    0        0     0         0  ...     0       0      0   \n",
       "4     0         0    0        0     0         0  ...     0       0      0   \n",
       "\n",
       "   devouring  verify  others  facebook  workage  pool  unjalur  \n",
       "0          0       0       0         0        0     0        0  \n",
       "1          0       0       0         0        0     0        0  \n",
       "2          0       0       0         0        0     0        0  \n",
       "3          0       0       0         0        0     0        0  \n",
       "4          0       0       0         0        0     0        0  \n",
       "\n",
       "[5 rows x 7804 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "acc38651",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAKSHMI\\AppData\\Local\\Temp\\ipykernel_20816\\2287711286.py:2: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  spam_messages = training_set_clean[training_set_clean['type'] == 'spam']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Isolating spam and ham messages first\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spam_messages \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_set_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraining_set_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m ham_messages \u001b[38;5;241m=\u001b[39m training_set_clean[training_set_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mham\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\frame.py:3492\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3490\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) DataFrame?\u001b[39;00m\n\u001b[0;32m   3491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, DataFrame):\n\u001b[1;32m-> 3492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3494\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\frame.py:10955\u001b[0m, in \u001b[0;36mDataFrame.where\u001b[1;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[0;32m  10942\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(\n\u001b[0;32m  10943\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcond\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m  10944\u001b[0m )\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10953\u001b[0m     try_cast\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[0;32m  10954\u001b[0m ):\n\u001b[1;32m> 10955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_cast\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\generic.py:9308\u001b[0m, in \u001b[0;36mNDFrame.where\u001b[1;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[0;32m   9300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m try_cast \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   9301\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   9302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry_cast keyword is deprecated and will be removed in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   9303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   9304\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   9305\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   9306\u001b[0m     )\n\u001b[1;32m-> 9308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\generic.py:9075\u001b[0m, in \u001b[0;36mNDFrame._where\u001b[1;34m(self, cond, other, inplace, axis, level, errors)\u001b[0m\n\u001b[0;32m   9072\u001b[0m     cond \u001b[38;5;241m=\u001b[39m cond\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   9074\u001b[0m cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcond \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m cond\n\u001b[1;32m-> 9075\u001b[0m cond \u001b[38;5;241m=\u001b[39m \u001b[43mcond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   9077\u001b[0m \u001b[38;5;66;03m# try to align with other\u001b[39;00m\n\u001b[0;32m   9078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, NDFrame):\n\u001b[0;32m   9079\u001b[0m \n\u001b[0;32m   9080\u001b[0m     \u001b[38;5;66;03m# align with me\u001b[39;00m\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\util\\_decorators.py:324\u001b[0m, in \u001b[0;36mrewrite_axis_style_signature.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any]:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\frame.py:4804\u001b[0m, in \u001b[0;36mDataFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4802\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4803\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 4804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreindex(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\generic.py:4966\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   4965\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 4966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m   4968\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\frame.py:4617\u001b[0m, in \u001b[0;36mDataFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   4615\u001b[0m columns \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   4616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4617\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\n\u001b[0;32m   4619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4621\u001b[0m index \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   4622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\frame.py:4662\u001b[0m, in \u001b[0;36mDataFrame._reindex_columns\u001b[1;34m(self, new_columns, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   4649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reindex_columns\u001b[39m(\n\u001b[0;32m   4650\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4651\u001b[0m     new_columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4657\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4658\u001b[0m ):\n\u001b[0;32m   4659\u001b[0m     new_columns, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[0;32m   4660\u001b[0m         new_columns, method\u001b[38;5;241m=\u001b[39mmethod, level\u001b[38;5;241m=\u001b[39mlevel, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[0;32m   4661\u001b[0m     )\n\u001b[1;32m-> 4662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_with_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4663\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\generic.py:5032\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   5029\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ensure_platform_int(indexer)\n\u001b[0;32m   5031\u001b[0m \u001b[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[39;00m\n\u001b[1;32m-> 5032\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5035\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_dups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5039\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5040\u001b[0m \u001b[38;5;66;03m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[0;32m   5041\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\internals\\managers.py:679\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;66;03m# some axes don't allow reindexing with dups\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dups:\n\u001b[1;32m--> 679\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_can_reindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\AnacondaS\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4107\u001b[0m, in \u001b[0;36mIndex._validate_can_reindex\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;66;03m# trying to reindex on an axis with duplicates\u001b[39;00m\n\u001b[0;32m   4106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 4107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam_messages = training_set_clean[training_set_clean['type'] == 'spam']\n",
    "ham_messages = training_set_clean[training_set_clean['type'] == 'ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "68c4913f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of       type                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5570   ham               Will Ã¼ b going to esplanade fr home?\n",
       "5571   ham  Pity, * was in mood for that. So...any other s...\n",
       "5572   ham  The guy did some bitching but I acted like i'd...\n",
       "5573   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5574 rows x 2 columns]>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e457b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc3334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04121715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
